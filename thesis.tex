\documentclass{ucbthesis}

\setcounter{secnumdepth}{2}

\usepackage{nicefrac}
\usepackage{siunitx}
\sisetup{
    math-micro=\ensuremath{\mu},
    text-micro=\ensuremath{\mu},
    range-units=single,
}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}

\newcommand{\nuebar}{\ensuremath{\bar{\nu}_e}}
\newcommand{\thetaot}{\ensuremath{\theta_{13}}}
\newcommand{\tc}{\ensuremath{T_c}}
\newcommand{\fold}[1]{\ensuremath{#1}-fold}
\newcommand{\mev}{\mega\electronvolt}
 
\begin{document}

\chapter{Neutrino Oscillations}

\chapter{The Daya Bay Reactor Antineutrino Experiment}

\chapter{Calibration}

\chapter{Reconstruction}

\chapter{Event selection}

The event selection extracts pairs of signals
in the data stream that have properties expected of
IBD interactions: a prompt positron annihilation
followed by a delayed neutron capture on hydrogen.
At each step in the selection, it is critical that
differences in cut efficiency between the ADs are both
minimized and quantified so that any observed near-far difference
can be justifiably attributed to oscillations.

Certain backgrounds, namely muons, PMT flashers, and accidental coincidences,
are frequent enough that their characterization, veto, and/or subtraction
are handled as part of the event selection process.
I will briefly describe these backgrounds inline
but leave detailed descriptions and studies
to the appropriate sections in \cref{ch:background}.

\section{Initial data preparation}

Before physics events are identified,
the data is cleaned in two ways.
First, cosmogenic muon events are identified
and a time window after each muon is vetoed
to allow for any spallation products or activated nuclei to decay
without contaminating the IBD signal (\cref{sec:muonveto}).
Second, occurrences of PMT light emission, known as ``flashers,''
are also vetoed and removed from the data stream (\cref{subsec:flashers}).

\section{Coincidence selection}
\label{sec:coincidence}

The first step in the event selection is
to group together signals that are close together in time
into ``coincidence groups.''
Each \SI{1}{\micro\second} triggered readout window
with reconstructed energy above \SI{1.5}{\mega\electronvolt}
is identified as an ``AD event''
and is a potential coincidence candidate.
Because of the nonzero length of the readout window,
AD events occurring closer together than \SI{1}{\micro\second}
are not necessarily separate physical events.
Consequently, during the coincidence grouping process,
the coincidence search window begins \SI{1}{\micro\second}
after the initial AD event.
Coincidence groups are constructed by repeating the following steps
until the data file is exhausted:

\begin{enumerate}
    \item Find the next AD event.
        This AD event will be the ``prompt'' event of the coincidence group.
    \item Find all subsequent AD events within the desired coincidence time \tc.
        If a muon event is encountered within \tc,
        veto the entire coincidence group starting with the prompt event.
        (This additional vetoed time is accounted for in the muon veto efficiency.)
    \item Group these events together with the prompt event
        to form the coincidence group.
    \item Skip to the next AD event that is not part of the coincidence group.
\end{enumerate}

The total number of AD events in the group
is the multiplicity of the group.
Because of the initial \SI{1}{\micro\second} gap,
the actual time interval covered by any given coincidence window is
$\tc - \SI{1}{\micro\second}$.
This analysis uses a coincidence search window of $\tc = \SI{1.5}{\milli\second}$.

A coincidence group with multiplicity $n$ is also referred to
as an \fold{n} coincidence.
If a prompt event has no subsequent AD events within \tc, it is
still a valid group, and is referred to as a \fold{1} coincidence.
Note that \fold{1} coincidences are somewhat but not strictly isolated
from other AD events.
Certainly there are no other AD events
within \tc{} \textit{after} the prompt event,
but there may be a \textit{preceding} AD event within \tc{}
if that event is part of a coincidence window
which ends before the prompt event in question.

Once the coincidence groups have been constructed,
the set of \fold{2} coincidences can be identified as
the preliminary set of IBD candidates,
albeit with background still present.
The \fold{1} coincidences can be identified as a subset
of the uncorrelated events, mostly radioactive decays,
that are also present in the data stream.
However, not all uncorrelated events end up in \fold{1} coincidences.
Sometimes an uncorrelated event will occur in close proximity to
a true IBD prompt-delayed pair, creating a \fold{3} coincidence.
These high-multiplicity coincidence groups are vetoed
with a small loss of efficiency (\cref{subsec:acc}).
More concerning is when two uncorrelated events
randomly occur in close proximity to each other,
creating a \fold{2} coincidence group that passes the high-multiplicity veto.
These so-called ``accidental'' coincidences
constitute the largest background within the set of \fold{2} coincidences.
The distance, time and energy cuts described below
are all motivated in large part by the need to reduce the accidental background.

\section{Distance and time cuts}

The distance and time distributions between prompt and delayed AD events
are different depending on the physical process producing those AD event pairs.
For example, the neutron produced during an IBD interaction
scatters within the liquid scintillator until being captured
by a hydrogen nucleus,
traveling a characteristic distance over a characteristic time.
On the other hand, two uncorrelated events have, by definition,
no particular connection between their physical locations
or their timings.

In practice, the characteristic distance for a neutron capture on hydrogen
is approximately \SI{200}{\milli\meter},
and the characteristic time delay is \SI{200}{\micro\second}.
For accidental coincidences, the characteristic distance is
the length scale of the AD, approximately \SI{3000}{\milli\meter},
and the time delay has a flat probability distribution
on the time scale used for the coincidence window ($\tc = \SI{1.5}{\milli\second}$).

For IBDs, the distance and time distributions are correlated
on an event-by-event basis.
% TODO briefly describe the dependence / correlation
A downside to this fact is that selecting events first using a distance cut
and then using a time cut, or vice versa,
would lead to partially correlated uncertainties on the cut efficiencies
that are difficult to characterize.
However, this correlation can be exploited to create a single cut
combining the distance and time that is straightforward to work with.

The exact combination of parameters for the cut was chosen based on observations
of the coincidence distance vs. coincidence time distributions.
This cut is known as the D-T cut and has the form

\begin{equation}
    \Delta r + v_0 \Delta t < \SI{800}{\milli\meter}
\end{equation}

where $v_0 = \frac{\SI{1000}{\milli\meter}}{\SI{600}{\micro\second}}$.
Applying this cut rejects the vast majority of accidental events
at a loss of approximately \SI{30}{\percent} of real IBDs.
The efficiency is measured after subtracting the accidental background
by comparing the number of events that pass the energy cuts and the D-T cut
with the number of events that pass the energy cuts
and a significantly relaxed D-T cut of \SI{3000}{\milli\meter}.
This latter quantity is assumed to be the total number of IBDs
since negligibly few IBDs have a D-T value anywhere close to \SI{3000}{\milli\meter}.

The AD-uncorrelated uncertainty for the efficiency
is determined from the data by examining the variation
in measured efficiency between the 4 near-hall ADs.
The far-hall ADs are excluded because
their statistical uncertainties are much larger than the
near-hall AD variation.
The AD-uncorrelated uncertainty of the D-T cut efficiency
is the half-range of the near-hall efficiencies: \num{0.0016} (absolute),
or approximately \SI{0.23}{\percent} (relative).

\section{Energy cuts}

\subsection{Prompt energy}
The prompt energy lower bound of \SI{1.5}{\mev}
is chosen to exclude a substantial fraction
of the low-energy uncorrelated events from radioactive decays.
In particular, X and Y decays have peaks around A and B \si{\mev}.
% TODO which isotopes?

The nominal efficiency of the prompt energy cut is estimated using Monte Carlo.
%TODO figure
However, because of the energy dependence of the \nuebar{} oscillations,
a different fraction of \nuebar{} will pass this cut
depending on the baseline between the reactor and the AD.
For example, at shorter baselines, low-energy \nuebar{}
are more likely to oscillate to other flavors, so that at the near halls,
there are fewer IBD events missed by the prompt energy cut,
thus raising the efficiency of the cut.
At the oscillation maximum, though, medium-energy \nuebar{},
around \SIrange{2}{3}{\mev}, are most likely to oscillate.
So a smaller fraction of IBDs will pass the cut,
and the efficiency will be lower.

Corrections for each AD--reactor pair should be computed
and weighted to arrive at each AD's final prompt energy efficiency.
Since the corrections to the efficiency depend on
the amplitude of \nuebar{} oscillations, they rely on knowledge of \thetaot.
(For example, there would be no corrections at all if \thetaot{} were $0$.)
To compute accurate corrections and, more importantly, an accurate value
for \thetaot{}, an iterative process is used.
Initially, no baseline-dependent correction is used and
a value for \thetaot{} is obtained.
That initial \thetaot{} is then used to compute baseline-dependent corrections,
and the updated efficiencies are used to compute an updated value for \thetaot{}.
This process is repeated until the \thetaot{} result converges.

The AD-uncorrelated uncertainty for the prompt energy lower bound
is dominated by differences in the energy scale between ADs.
Based on the analysis of the delayed energy spectrum in each AD
reported in \cref{subsec:delayed}, the energy scale
varies by less than \SI{0.5}{\percent} between ADs.
By applying a \SI{+-0.5}{\percent} variation to
the event energy in the Monte Carlo dataset,
the impact of the energy scale differences can be propagated
to the prompt energy efficiency.
The impact, and therefore the relative uncertainty on
the prompt energy efficiency, is observed to be \SI{0.1}{\percent}.

There is also a \SI{12}{\mega\electronvolt} upper bound for the prompt energy.
The reactor \nuebar{} spectrum falls steeply above \SI{8}{\mev},
so this cut has essentially \SI{100}{\percent} efficiency.

\subsection{Delayed energy}
\label{subsec:delayed}

Neutron capture on hydrogen releases a (monoenergetic)
\SI{2.22}{\mev} $\gamma$,
which means the cut values can be tuned to a narrow energy region
around that value.
However, while the prompt IBD positron essentially never escapes
from the liquid scintillator sensitive volume,
$\gamma$'s do regularly escape (a few percent of the time),
depositing less than their full energy in the liquid scintillator.
Both the tuning of the energy cut values
and the fraction of escaping $\gamma$'s are sensitive to
small variations in the geometry of the AD, energy reconstruction,
and scattering properties of $\gamma$'s in
both liquid scintillator and in acrylic.

The delayed energy cut bounds are identified based on
functional fits to each AD's delayed energy spectrum.
Specifically, the spectrum is generated by first applying
the prompt energy cut and D-T cut,
then statistically subtracting the accidental background (\cref{subsec:acc}).
Although the resulting spectrum still has some background,
the only remaining background processes also involve
neutron capture on hydrogen, and contribute to the same spectral shape
as the signal IBD process.

Each spectrum is fit with the calorimeter function, which models
a calorimetric response to a monoenergetic process with ``true''
energy $\mu$.
The modeled detector has an intrinsic and energy resolution $\sigma$
which applies a Gaussian smearing to the deposited energy.
($\sigma$ itself is independent of energy.)
While many events in this model deposit all of their energy into
the calorimeter, some events partially leak out.
The fraction of events that deposit their full energy in the detector
is referred to as the peak fraction $\alpha$.
The energy leakage is modeled as an exponential distribution
with characteristic energy scale $\lambda$.
The fit function itself is derived by starting with
the unsmeared model:

\begin{equation*}
    f_{unsmeared}(E) =
    \begin{cases}
        \alpha\delta(E-\mu) + (1-\alpha)\lambda e^{\lambda E}
        & 0 < E \leq \mu \\
        0 & E > \mu
    \end{cases}
\end{equation*}

This function is then convolved with a Gaussian
of width $\sigma$.

\begin{align*}
    f_{cal}    &= f_{unsmeared} \otimes \text{Gaussian} \\
    f_{cal}(E) &= \int_0^\mu dE' f_{unsmeared}(E') \cdot \text{Gaussian}(E'-E; \mu) \\
               &= \frac{1}{\sigma\sqrt{2\pi}}
               \left[
                   \alpha\int_0^\mu dE' e^{-\frac{(E'-E)^2}{2\sigma^2}} \delta(E'-\mu)
                   + (1-\alpha)\int_0^\mu dE' e^{-\frac{(E'-E)^2}{2\sigma^2}}
                   \lambda e^{\lambda E'}
               \right] \\
               &= \alpha\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(E-\mu)^2}{2\sigma^2}}
               + (1-\alpha)
               \frac{\lambda e^{\sigma^2\lambda^2+2\lambda E}}{e^{\lambda\mu}-1}
               \left[
                   \text{erf}
                   \left(
                       \frac{\mu-E-\sigma^2\lambda}{\sigma\sqrt{2}}
                   \right)
                   + \text{erf}
                   \left(
                       \frac{E + \sigma^2\lambda}{\sigma\sqrt{2}}
                   \right)
               \right]
\end{align*}
The entire result is normalized to unity
but can be scaled by an overall normalization $N$.

The eight delayed energy spectra with their fits are shown in Figure X. %TODO
Comparing the fitted parameters across ADs can be used
to measure the identicalness of the ADs.
Their values and relative differences are plotted in Figure X %TODO
and listed in Table Y. %TODO
In particular, Figure X(a) shows the relative difference %TODO
in the fitted peak $\mu$ across the ADs,
which is a measure of the energy scale variation.
The relative variation of \SI{+-0.5}{\percent}
is used to compute the AD-uncorrelated uncertainty
on the prompt energy cut efficiency.

The bounds for the delayed energy cut are computed
based on the fitted peak value and energy resolution
from the calorimter model. The energy criterion is

\begin{equation}
    \mu - 3\sigma < E < \mu + 3\sigma.
\end{equation}

This form was decided on even though the calorimeter function
is not symmetric, and even though in reality
the detector resolution changes with energy
(and therefore is not precisely modeled in the fit).
The values used for the energy bounds are listed in Table Y %TODO
and plotted in Figure X. %TODO

The absolute efficiency of the delayed energy cut
is measured using Monte Carlo.
The same fitting procedure with the calorimeter function
and the same $\mu \pm 3\sigma$ bounds are used to
estimate the fraction of neutron capture on hydrogen events
that pass the delayed energy cut.
The relative fraction of GdLS to LS also impacts the efficiency
because with more GdLS, fewer neutrons will capture on hydrogen.
This difference between the GdLS and LS regions is taken into account
when computing the absolute efficiency.

The AD-uncorrelated uncertainty on the efficiency
includes variations due to all the above effects:
geometrical variations, energy scale, different material properties,
Gd fraction, etc.
The overall variation between ADs is estimated using a general method
that compares the relative size between the peak and tail regions
of the delayed energy spectrum, as defined by the delayed energy cut,
across the ADs.
Intuitively, if the same fraction of events is in the peak region in each AD,
then the cut efficiency must have a small variation.
This intuition is visualized in the cartoon in Figure X. %TODO
In practice, it is easier to compare the number of events in the peak
to the number of events in an extended region that includes the tail.

For each AD's delayed energy spectrum, the peak region is defined as
the region passing the delayed energy cut, $\vert E-\mu \vert < 3\sigma$.
The extended region is the same for each AD:
$\SI{1.5}{\mev} < E < \SI{2.8}{\mev}$.
Using an AD-dependent definition for the peak region but
the same values for the extended creates
the desired sensitivity to variations in the energy scale.
Extending the lower bound down to \SI{1.5}{\mev} adds sensitivity to
anything that would change the shape of the tail
or the fraction of $\gamma$'s that (partially) escape from the AD.
The number of events in the peak region ($N_{peak}$)
and in the extended region ($N_{extended}$) for each AD
is given in Table Y. %TODO

If the delayed energy spectra have the same shape,
and if the calorimeter function is an appropriate fitting function
for determining the energy cut bounds,
then for some constant $b$ common to all ADs,
$N_{peak,\,i} = b N_{extended,\,i}$ for each AD $i$.
To test this model, an affine linear function is fit
to the values for each AD:

\begin{equation}
    N_{peak,\,pred,\,i} = a + b N_{extended,\,i},
\end{equation}

where $N_{peak,\,pred,\,i}$ is the fit function's prediction
of the actual event count in the peak ($N_{peak,\,i}$).
Plots of $N_{peak,\,i}$ vs. $N_{extended,\,i}$ and of the
relative deviation from the fitted line are shown in Figure X. %TODO
The fit value of $a$ should be $0$ if the simple model of
a linear scaling is correct.
Indeed, the fit value is $a = -321 \pm 387$
which is consistent with $0$.
The fit value for $b$ is $b = 0.9527 \pm 0.0015$,
which can be interpreted as a loose upper bound
on the delayed energy cut efficiency.
The final value for the AD-uncorrelated uncertainty
of the delayed energy efficiency is taken to be
the half-range of the relative deviations
of the near-hall ADs from the fitted line: \SI{0.11}{\percent}.
The far-hall ADs have much higher statistical uncertainty
(relative uncertainty $\approx\SI{0.5}{\percent}$),
and therefore larger fluctuations.
Validation that the far-hall ADs are not
substantially different from the near-hall ADs is obtained by
summing the counts for all 4 far-hall ADs to get
a combined relative deviation for the far hall of \SI{0.018}{\percent},
well within the uncertainty derived from the near halls.




\chapter{Background}
\label{ch:background}

\section{Muon veto}
\label{sec:muonveto}

\section{Uncorrelated background}

\subsection{PMT light emission / flashers}
\label{subsec:flashers}

\subsection{Accidental coincidences}
\label{subsec:acc}

\chapter{Measurement of \texorpdfstring{$\thetaot$}{theta13}}

\chapter{Conclusions}

\appendix
\chapter{Derivation of singles and accidental rate formulas}
\label{ap:singlesformula}

The formulas used for computing the rate of uncorrelated events
and the rate of accidental coincidences
are based on the statistical interpretation
of the coincidence grouping algorithm.
As mentioned in \cref{sec:coincidence}, coincidence groups are formed
by repeating the following steps:

\begin{enumerate}
    \item Find the next AD event.
        This AD event will be the ``prompt'' event of the coincidence group.
    \item Find all subsequent AD events within the desired coincidence time \tc.
        If a muon event is encountered within \tc,
        veto the entire coincidence group starting with the prompt event.
        (This additional vetoed time is accounted for in the muon veto efficiency.)
    \item Group these events together with the prompt event
        to form the coincidence group.
    \item Skip to the next AD event that is not part of the coincidence group.
\end{enumerate}

A reasonable and leading question to ask is,
Given the rate and correlations of AD events and muons,
what is the expected rate of \fold{1} or \fold{2}
(or, generally, \fold{n}) coincidences when using
this particular coincidence grouping procedure?

The model used for Daya Bay data is that muons
and the ``single'' or ``uncorrelated''
events that cause accidental backgrounds and
multiplicity vetos are truly uncorrelated and described by
Poisson statistics with rates of $R_\mu$ and $R_s$, respectively.
True correlated processes such as IBDs are neglected in this model
because their rate is $\sim 4$ orders of magnitude lower than $R_s$.

\section{Detailed derivation}

The derivation of the rate formulas can be broken down into three
conceptual parts which are actually factors that, when multiplied together,
give the desired rate:

\begin{enumerate}
    \item $R_s$: How many opportunities are there for a coincidence window to start?
    \item $P_{\text{start}}$: What is the probability that a given event will start a
        coincidence window (as opposed to falling within
        an existing coincidence window)?
    \item $P_{\text{mult}}$: Once started, what is the probability
        that the coincidence window has the desired multiplicity?
\end{enumerate}

Factors 1 and 3 are straightforward.
There are as many opportunities for coincidence windows to start
as there are uncorrelated events, so factor 1 is simply
the underlying uncorrelated event rate $R_s$.
(Again, neglecting IBDs and other backgrounds.)
Factor 3 is the probability that $n-1$ additional events occur
within the specified time interval of size \tc{}
(for a desired multiplicity of $n$), when those events occur with a rate $R_s$.
This is just the Poisson probability with mean $R_s\tc$:

\begin{equation}
    \text{Poisson}(n\vert R_s\tc) = \frac{\left(R_s\tc\right)^n}{n!}e^{-R_s\tc}.
\end{equation}
Hence, whatever is derived for Factor 2 below should be multiplied by
$R_s\text{Poisson}(n\vert R_s\tc)$ to obtain the formula for
an \fold{n} coincidence.

Factor 2, unfortunately, is more involved.
The probability that a given random AD event
is properly situated to start a new coincidence window depends on
the likelihood of other events to be far enough away in the past.
An equivalent formulation is to pick a time uniformly at random
(from among the times that lie outside muon veto windows)
and determine the probability that, if a new AD event were created
at that time, the event would \textit{not} lie within another
coincidence window.
This probability can be calculated by considering
a set of mutually exclusive situations
and summing the individual probability for each situation.
Three situations contribute the majority of the probability;
the next-most-likely situation is briefly examined in \cref{ap:singlesprecision}
to demonstrate its negligible likelihood.
The three situations are:

\renewcommand{\labelenumi}{(\alph{enumi})}
\begin{enumerate}
    \item There are no muons and no other AD events
        within \tc{} before the chosen event.
        This probability is computed as two Poisson probabilities of
        no events with rate $R_s$ in time \tc,
        and no events with rate $R_\mu$ in time \tc:

        \begin{align*}
            P_a &= \text{Poisson}(0\vert R_s\tc)\cdot\text{Poisson}(0\vert R_\mu\tc) \\
                &= e^{-\left(R_s + R_\mu\right)\tc}
        \end{align*}

    \item There is another AD event, E1, within \tc{}
        before the chosen event, E0,
        but it lies within an earlier coincidence window.
        This is significantly more complicated to calculate.
        Let $t_1$ refer to the time difference between E1 and E0.
        The time between uncorrelated events follows an exponential distribution,
        so the probability of E1 occuring between $t_1$ and $t_1+dt_1$ is
        $dt_1\,R_se^{-R_st_1}$.
        Examining the possibilities for this earlier coincidence window,
        which contains E1 but not E0,
        it is clear that the end of the window must be before E0,
        but not earlier than $t_1$ before E0, so that it still contains E1.
        Thus the time difference between the end of the window and E0
        must have a value between 0 and $t_1$.
        Since the coincidence window is a fixed duration \tc{},
        the event E2 which started the earlier window must occur
        between \tc{} and $\tc + t_1$ before E0.
        If more than one event occurs within that time interval,
        the requirements for the coincidence window would still be satisfied.
        The probability of at least one event (E2, E3, \ldots) occuring
        between \tc{} and $\tc + t_1$ before E0,
        and thus creating the desired coincidence window, is

        \begin{equation*}
            1-\text{Poisson}(0\vert R_s t_1) = 1-e^{-R_st_1}.
        \end{equation*}

        Finally, there must be no muon veto window ending within this time range
        between E2 and E0, which has duration $\tc + t_1$,
        leading to the Poisson probability $e^{-R_\mu(\tc+t_1)}$.
        Integrating over values for $t_1$ between $0$ and \tc{}
        gives the final result for this probability:

        \begin{align*}
            P_b &= \int_0^{\tc} dt_1\,R_se^{-R_st_1}
            \left(
                1 - e^{-R_s t_1}
            \right)
            e^{-R_\mu(\tc+t_1)} \\
                &= \frac{R_s}{R_s+R_\mu} e^{-R_\mu\tc}
                \left(
                    1 - e^{-(R_s + R_\mu)\tc}
                \right)
                - \frac{R_s}{2R_s + R_\mu} e^{-R_\mu\tc}
                \left(
                    1 - e^{-(2R_s + R_\mu)\tc}
                \right).
        \end{align*}
    \item There is a muon veto window ending within \tc{}
        before the chosen event (and no other AD events).
        If the time interval between the muon and the chosen event is $t_\mu$,
        then the probability of the most recent muon being between
        $t_\mu$ and $t_\mu + dt_\mu$ follows the exponential distribution,
        $dt_\mu\,R_\mu e^{-R_\mu t_\mu}$.
        The probability of no other AD events in this interval is just
        $\text{Poisson}(0\vert R_s t_\mu)$.
        Integrating over $t_\mu$ between $0$ and \tc{} gives the final result:

        \begin{align*}
            P_c &= \int_0^{\tc} dt_\mu\,R_\mu e^{-R_\mu t_\mu} e^{-R_s t_\mu} \\
                &= \frac{R_\mu}{R_s + R_\mu}
                \left(
                    1 - e^{-(R_s + R_\mu)\tc}
                \right).
        \end{align*}

\end{enumerate}
The final formula used for the probability of starting a coincidence window
$P_{\text{start}}$ is the sum of these three terms.

Combining this probability with the other factors gives the following formula
for the probability of an \fold{n} coincidence assuming only
uncorrelated AD events and uncorrlated muons,
neglecting the possibility of correlated pairs or muon-correlated events:

\begin{align} \label{eq:rnfold}
    \begin{split}
        R_{\text{\fold{n}}}
          &= R_s P_{\text{start}} \text{Poisson}(0\vert R_s\tc) \\
          &= R_s \frac{(R_s\tc)^n}{n!} e^{-R_s\tc}
          \left(
              e^{-(R_s + R_\mu)\tc} +
              \frac{R_s}{R_s+R_\mu} e^{-R_\mu\tc}
              \left(
                  1 - e^{-(R_s + R_\mu)\tc}
              \right)
          \right. \\
          &\ \ \left. - \frac{R_s}{2R_s + R_\mu} e^{-R_\mu\tc}
          \left(
              1 - e^{-(2R_s + R_\mu)\tc}
          \right) +
          \frac{R_\mu}{R_s + R_\mu}
          \left(
              1 - e^{-(R_s + R_\mu)\tc}
          \right)
      \right)
    \end{split}
\end{align}

\section{Precision}
\label{ap:singlesprecision}

Values for the three components of $P_{\text{start}}$
under typical near-hall and far-hall scenarios
are given in \cref{tab:pstartcomponents}.
As may be expected given the low singles and muon
rates compared to $\nicefrac{1}{\tc}$,
the most likely situation is that any given AD event
is isolated, corresponding to $P_a$.
The next-most-likely scenario is that the given AD event
is relatively close to a preceding muon veto window,
corresponding to $P_c$.
Since the situation corresponding to $P_b$ requires
not only the given AD event but also 2 others,
all within a time interval shorter than $2\tc$,
it is not surprising that $P_b\ll P_c (< P_a)$
at both the near and far sites.

While $P_a$ and $P_c$ are exact mathematical representations
of the corresponding physical scenarios,
$P_b$ is just an approximation stemming from the fact that
the description of the scenario is incomplete.
Recall that $P_b$ is the probability that the given event E0
is preceded by both E1 within \tc{} and E2 further in the past,
configured in such a way that E1 lies inside E2's coincidence window,
and E0 lies outside it.
Missing from this description is that E2 itself must not lie
within a previous event's coincidence window.
(If it did, then E1 would start its own coincidence window
that contains E0.)
This could happen for 2 reasons: either (1) there are no events
within \tc{} before E2, (2) there is an event E3 within \tc{}
of E2, and also another event E4 before E3 such that
E3 lies within E4's coincidence window but E2 does not,
or (3) there is a muon but no AD events within \tc{} before E2.
These three scenarios are exactly the same as the original three
describing the possibilities for E0.
It is not too difficult to realize that there is an infinite nesting
of scenarios based around (2), that is, around $P_b$.

In other words, $P_{\text{start}}$ should really be computed as

\begin{align*}
    \begin{split}
        P_{\text{start, better}} &= P_a + P_c + P_b(P_a +
            P_c + P_b(P_a+P_c+P_b(\cdots))) \\
                                 &= P_a + P_c + P_bP_{\text{start}},
    \end{split}
\end{align*}

where the second line is obtained by observing that the value
of the infinitely-nested terms in parenteses is simply $P_{\text{start, better}}$.
Solving for $P_{\text{start, better}}$ we obtain the slightly better approximation

\begin{equation}
    P_{\text{start, better}} = \frac{P_a + P_c}{1-P_b}.
\end{equation}

For both the near and far halls, the relative difference
between $P_{\text{start}}$ and $P_{\text{start, better}}$ is $O(10^{-5})$,
so even though $P_{\text{start, better}}$ is still not exact,
further adjustments to it are not necessary.
Why is it not exact?
The description of the physical scenario is once more not complete.
Technically, an AD event \textit{could} occur before E2 in such a way
that E0 would still be able to start a new coincidence window.
This is possible if the hypothetical E3 is very close before E2,
so that E3's coincidence window still contains E1.
The probability of E3 and E2 being ``close enough together''
is strictly less than the probability of E3 occuring anywhere
within \tc{} of E2, which is $\text{Poisson}(1\vert R_s\tc) = 0.028$.
So this correction would be less than a \SI{3}{\percent} correction
to the already negligible $10^{-5}$ correction on $P_{\text{start}}$.

\begin{table}[ht]
    \centering
    \begin{tabular}[t]{lll}
        \hline
        & Near hall & Far hall \\
        \hline
        \tc & \multicolumn{2}{c}{\SI{1500}{\micro\second}} \\
        $R_s$ & \multicolumn{2}{c}{\SI{19}{\hertz}} \\
        $R_\mu$ & \SI{200}{\hertz} & \SI{16}{\hertz} \\
        \hline
        $P_a$ & \num{0.72000} & \num{0.94885} \\
        $P_b$ & \num{0.00024} & \num{0.00038} \\
        $P_c$ & \num{0.25571} & \num{0.02338} \\
        $P_{\text{start}}$ & \num{0.97595} & \num{0.97261} \\
        $P_{\text{start, better}}$ & \num{0.97594} & \num{0.97260} \\
        \hline
    \end{tabular}
    \caption{Values for each component of $P_{\text{start}}$
    for typical $R_\mu$ and $R_s$ at the near and far halls.}
    \label{tab:pstartcomponents}
\end{table}




\end{document}
